{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052aaa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "from better_profanity import profanity\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935eb1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_character_count(word_list):\n",
    "    count_dict = defaultdict(list)\n",
    "    for word in word_list:\n",
    "        count_dict[len(word)].append(word)\n",
    "    return count_dict\n",
    "\n",
    "def plot_word_counts(counts_dict):\n",
    "    max_count = max(counts_dict.keys())\n",
    "    x = list(range(max_count+1))\n",
    "    y = []\n",
    "    for c in x:\n",
    "        y.append(len(counts_dict[c]))\n",
    "    plt.bar(x, y)\n",
    "    plt.title(\"Distribution of word size\")\n",
    "    plt.xlabel(\"Character count\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc2a93a",
   "metadata": {},
   "source": [
    "# Source 1\n",
    "(https://cs.stanford.edu/~knuth/sgb-words.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb312f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list_1 = []\n",
    "with open(\"sgb-words.txt\", \"r\") as file:\n",
    "    words_list_1 = file.read().split()\n",
    "len(words_list_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11f435",
   "metadata": {},
   "source": [
    "# Source 2: SCOWL (Spell Checker Oriented Word Lists) dataset\n",
    "(http://wordlist.aspell.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_read = [\n",
    "    \"english-words.10\",\n",
    "    \"english-words.20\",\n",
    "    \"english-words.35\",\n",
    "    \"english-words.40\",\n",
    "    \"english-words.50\",\n",
    "    \"english-words.55\",\n",
    "    \"english-words.60\",\n",
    "    \"english-words.70\",\n",
    "    \"american-words.10\",\n",
    "    \"american-words.20\",\n",
    "    \"american-words.35\",\n",
    "    \"american-words.40\",\n",
    "    \"american-words.50\",\n",
    "    \"american-words.55\",\n",
    "    \"american-words.60\",\n",
    "    \"american-words.70\",\n",
    "    \"british-words.10\",\n",
    "    \"british-words.20\",\n",
    "    \"british-words.35\",\n",
    "    \"british-words.40\",\n",
    "    \"british-words.50\",\n",
    "    \"british-words.55\",\n",
    "    \"british-words.60\",\n",
    "    \"british-words.70\",\n",
    "    \"british_z-words.10\",\n",
    "    \"british_z-words.20\",\n",
    "    \"british_z-words.35\",\n",
    "    \"british_z-words.40\",\n",
    "    \"british_z-words.50\",\n",
    "    \"british_z-words.55\",\n",
    "    \"british_z-words.60\",\n",
    "    \"british_z-words.70\",\n",
    "    \"special-hacker.50\"\n",
    "]\n",
    "\n",
    "folder_path = \"scowl-2020.12.07/final/\"\n",
    "\n",
    "words_list_2 = []\n",
    "\n",
    "for filename in files_to_read:\n",
    "    with open(folder_path + filename, \"r\", encoding=\"iso-8859-1\") as file:\n",
    "        words = file.read().split()\n",
    "        words_list_2.extend(words)\n",
    "\n",
    "len(words_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c009be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code generated by Perplexity\n",
    "def is_lowercase(word):\n",
    "    # only aâ€“z, one or more chars\n",
    "    return re.fullmatch(r\"[a-z]+\", word) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea80b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list_2 = list(set([w for w in words_list_2 if is_lowercase(w)]))\n",
    "len(words_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53677e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list_final = list(set(words_list_1).union(set(words_list_2)))\n",
    "len(words_list_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe84c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = split_by_character_count(words_list_final)\n",
    "plot_word_counts(count_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f64ad",
   "metadata": {},
   "source": [
    "## Remove profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e61134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "profanity_df = pd.read_csv(\"profanity_en.csv\")\n",
    "profanity_df_words_set = (set(profanity_df['text'])\n",
    "                           .union(set(profanity_df['canonical_form_1']))\n",
    "                           .union(set(profanity_df['canonical_form_2']))\n",
    "                           .union(set(profanity_df['canonical_form_3'])))\n",
    "len(profanity_df_words_set)\n",
    "\n",
    "# Library: better_profanity\n",
    "profanity.load_censor_words()\n",
    "\n",
    "def is_profane(word):\n",
    "    return profanity.contains_profanity(word) or word in profanity_df_words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration 1: generate a list of profane words\n",
    "profane_words = [word for word in words_list_final if is_profane(word)]\n",
    "len(profane_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration 2: use spaCy to identify lemmas\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "profane_lemmas_set = {token.lemma_.lower() for word in profane_words for token in nlp(word)}\n",
    "len(profane_lemmas_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe0777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_profane_iter2(word):\n",
    "    doc = nlp(word.lower())\n",
    "    return any(token.lemma_ in profane_lemmas_set for token in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7634ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "profane_words_final = [word for word in words_list_final if is_profane_iter2(word)]\n",
    "len(profane_words_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9405f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the words list\n",
    "\n",
    "words_list_final = list(set(words_list_final).difference(profane_words_final))\n",
    "len(words_list_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f290516c",
   "metadata": {},
   "source": [
    "## Save the list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df8e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../words_v1.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(w + \"\\n\" for w in words_list_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file to verify\n",
    "with open(\"../words_v1.txt\", \"r\") as file:\n",
    "    temp = file.read().split()\n",
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7581c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
